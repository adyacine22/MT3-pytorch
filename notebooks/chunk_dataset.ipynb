{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505a357d",
   "metadata": {},
   "source": [
    "1. **Per-batch tensor footprint**\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{bytes}_{\\text{wave}} &= \\text{chunk\\_samples} \\times 4 \\\\\n",
    "   \\text{bytes}_{\\text{token}} &= \\text{max\\_token\\_length} \\times 4 \\\\\n",
    "   \\text{batch\\_tensor\\_MB} &= \\frac{\\text{batch\\_size} \\times \\left(\\text{bytes}_{\\text{wave}} + \\text{bytes}_{\\text{token}}\\right)}{1024^2}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "2. **Inflight batches**\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{inflight} &= \\text{prefetch\\_factor} \\times \\min(\\text{num\\_workers}, \\text{batch\\_size}) \\\\\n",
    "   \\text{BatchTensors}_{\\text{total}} &= \\text{batch\\_tensor\\_MB} \\times \\left(1 + \\text{inflight}\\right)\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "3. **Temperature-weighted shard sizes**\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   w_i &=\n",
    "     \\begin{cases}\n",
    "       \\text{count}_i^{1 / \\text{temperature}} & \\text{if temperature} \\neq \\text{None} \\\\\n",
    "       \\text{count}_i & \\text{otherwise}\n",
    "     \\end{cases} \\\\\n",
    "   \\tilde{w}_i &= \\frac{w_i}{\\sum_j w_j} \\\\\n",
    "   \\text{avg\\_shard\\_MB} &= \\sum_i \\tilde{w}_i \\cdot \\text{mean\\_shard\\_size}_i\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "4. **Temperature-weighted MIDI sizes**\n",
    "\n",
    "   $$\n",
    "   \\text{avg\\_midi\\_MB} = \\sum_i \\tilde{w}_i \\cdot \\text{mean\\_noteseq\\_size}_i\n",
    "   $$\n",
    "\n",
    "5. **Cache footprint per worker**\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{ShardCache}_{\\text{per}} &= \\text{shard\\_cache\\_size} \\times \\text{avg\\_shard\\_MB} \\\\\n",
    "   \\text{MidiCache}_{\\text{per}} &= \\text{max\\_midi\\_cache\\_size} \\times \\text{avg\\_midi\\_MB}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "6. **Cache total across workers**\n",
    "\n",
    "   $$\n",
    "   \\text{Cache}_{\\text{total}} = \\text{num\\_workers} \\times \\left(\\text{ShardCache}_{\\text{per}} + \\text{MidiCache}_{\\text{per}}\\right)\n",
    "   $$\n",
    "\n",
    "7. **Peak RAM estimate**\n",
    "\n",
    "   $$\n",
    "   \\text{PeakRAM}_{\\text{total}} \\approx \\text{BatchTensors}_{\\text{total}} + \\text{Cache}_{\\text{total}}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3fdeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/ahmedyz/miniconda3/envs/mt3-pytorch/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import statistics\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "import note_seq  # pip install note-seq\n",
    "import pandas as pd\n",
    "\n",
    "from configs import load_project_config\n",
    "\n",
    "\n",
    "def estimate_peak_loader_ram(\n",
    "    *,\n",
    "    manifest_path: str | Path | None = None,\n",
    "    batch_size: int = 16,\n",
    "    num_workers: int = 8,\n",
    "    prefetch_factor: int = 2,\n",
    "    shard_cache_size: int = 32,\n",
    "    max_midi_cache_size: int = 2048,\n",
    "    split: str | Iterable[str] | None = None,\n",
    "    temperature: float | None = None,\n",
    "    shard_samples_per_dataset: int = 50,\n",
    "    midi_samples_per_dataset: int = 25,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate peak RAM for the MT3 dataloader (tensors + shard/MIDI caches).\n",
    "\n",
    "    Returns a dict with all intermediate values (in MiB) and the final estimate.\n",
    "    \"\"\"\n",
    "    cfg = load_project_config()\n",
    "    manifest_path = Path(manifest_path or cfg[\"paths\"][\"cache\"][\"chunk_manifest\"])\n",
    "    manifest = pd.read_parquet(manifest_path)\n",
    "    if split is not None:\n",
    "        allowed = {split} if isinstance(split, str) else set(split)\n",
    "        manifest = manifest[manifest[\"split\"].isin(allowed)].reset_index(drop=True)\n",
    "    if manifest.empty:\n",
    "        raise ValueError(\"Filtered manifest is empty; cannot estimate RAM.\")\n",
    "\n",
    "    # --- 1) Per-batch tensor footprint --------------------------------------\n",
    "    chunk_samples = cfg[\"audio\"][\"features\"][\"chunk_samples\"]\n",
    "    max_tokens = cfg[\"symbolic\"][\"tokenizer\"].get(\"max_token_length\", 1024)\n",
    "    bytes_wave = chunk_samples * 4  # float32\n",
    "    bytes_token = max_tokens * 4    # int32\n",
    "    batch_tensor_mb = batch_size * (bytes_wave + bytes_token) / 1024**2\n",
    "    print(f\"Estimated per-batch tensor footprint: {batch_tensor_mb:.2f} MiB\")   \n",
    "\n",
    "    # --- 2) Inflight batches -------------------------------------------------\n",
    "    inflight = prefetch_factor * min(num_workers, batch_size)\n",
    "    tensors_total = batch_tensor_mb * (1 + inflight)\n",
    "    print(f\"Estimated total tensor footprint (including inflight={inflight} batches): {tensors_total:.2f} MiB\")\n",
    "\n",
    "    # --- 3) Dataset weights (temperature-aware) ------------------------------\n",
    "    counts = manifest[\"dataset\"].value_counts().to_dict()\n",
    "    if temperature:\n",
    "        weights_raw = {ds: count ** (1.0 / temperature) for ds, count in counts.items()}\n",
    "    else:\n",
    "        weights_raw = counts\n",
    "    weight_norm = sum(weights_raw.values())\n",
    "    weights = {ds: val / weight_norm for ds, val in weights_raw.items()}\n",
    "    print(f\"Dataset weights (temperature={temperature}): {weights}\")\n",
    "    # --- helpers -------------------------------------------------------------\n",
    "    def sample_mean_file_size(paths: pd.Series, k: int) -> float:\n",
    "        paths = paths.dropna()\n",
    "        if paths.empty:\n",
    "            return 0.0\n",
    "        sample = paths.sample(min(k, len(paths)), random_state=0)\n",
    "        sizes = []\n",
    "        for raw in sample:\n",
    "            p = Path(raw)\n",
    "            if p.is_file():\n",
    "                sizes.append(p.stat().st_size / 1024**2)\n",
    "        return statistics.mean(sizes) if sizes else 0.0\n",
    "\n",
    "    def sample_mean_noteseq_size(paths: pd.Series, k: int) -> float:\n",
    "        paths = paths.dropna()\n",
    "        if paths.empty:\n",
    "            return 0.0\n",
    "        sample = paths.sample(min(k, len(paths)), random_state=0)\n",
    "        sizes = []\n",
    "        for raw in sample:\n",
    "            p = Path(raw)\n",
    "            if not p.is_file():\n",
    "                continue\n",
    "            try:\n",
    "                ns = note_seq.midi_file_to_note_sequence(str(p))\n",
    "                sizes.append(len(ns.SerializeToString()) / 1024**2)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return statistics.mean(sizes) if sizes else 0.0\n",
    "\n",
    "    # --- 3/4) Temperature-weighted averages ---------------------------------\n",
    "    avg_shard_mb = 0.0\n",
    "    avg_midi_mb = 0.0\n",
    "    for dataset, weight in weights.items():\n",
    "        subset = manifest[manifest[\"dataset\"] == dataset]\n",
    "        shard_mean = sample_mean_file_size(\n",
    "            subset.loc[subset[\"chunk_storage\"] == \"per_track\", \"chunk_shard_path\"],\n",
    "            shard_samples_per_dataset,\n",
    "        )\n",
    "        midi_mean = sample_mean_noteseq_size(\n",
    "            subset[\"midi_path\"],\n",
    "            midi_samples_per_dataset,\n",
    "        )\n",
    "        avg_shard_mb += weight * shard_mean\n",
    "        avg_midi_mb += weight * midi_mean\n",
    "    print(f\"Estimated average shard size: {avg_shard_mb:.2f} MiB\")\n",
    "    print(f\"Estimated average MIDI size: {avg_midi_mb:.2f} MiB\")\n",
    "\n",
    "    # --- 5) Cache footprint per worker --------------------------------------\n",
    "    shard_cache_per = shard_cache_size * avg_shard_mb\n",
    "    midi_cache_per = max(0, max_midi_cache_size or 0) * avg_midi_mb\n",
    "    print(f\"Estimated shard cache per worker: {shard_cache_per:.2f} MiB\")\n",
    "    print(f\"Estimated MIDI cache per worker: {midi_cache_per:.2f} MiB\")\n",
    "\n",
    "    # --- 6) Cache total across workers --------------------------------------\n",
    "    cache_total = num_workers * (shard_cache_per + midi_cache_per)\n",
    "    print(f\"Estimated total cache footprint across workers: {cache_total:.2f} MiB\")\n",
    "    # --- 7) Final estimate ---------------------------------------------------\n",
    "    peak_ram = tensors_total + cache_total\n",
    "    print(f\"Estimated peak dataloader RAM: {peak_ram:.2f} MiB\")\n",
    "\n",
    "    return {\n",
    "        \"batch_tensor_mb\": batch_tensor_mb,\n",
    "        \"inflight_batches\": inflight,\n",
    "        \"BatchTensors_total\": tensors_total,\n",
    "        \"avg_shard_mb\": avg_shard_mb,\n",
    "        \"avg_midi_mb\": avg_midi_mb,\n",
    "        \"ShardCache_per_worker\": shard_cache_per,\n",
    "        \"MidiCache_per_worker\": midi_cache_per,\n",
    "        \"Cache_total\": cache_total,\n",
    "        \"PeakRAM_total\": peak_ram,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a41d6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated per-batch tensor footprint: 33.00 MiB\n",
      "Estimated total tensor footprint (including inflight=256 batches): 8481.00 MiB\n",
      "Dataset weights (temperature=3.3333333333333335): {'slakh_stem': 0.48384983409481314, 'maestro': 0.2780256627908883, 'slakh_full_mix': 0.23812450311429859}\n",
      "Estimated average shard size: 14.16 MiB\n",
      "Estimated average MIDI size: 0.16 MiB\n",
      "Estimated shard cache per worker: 1812.89 MiB\n",
      "Estimated MIDI cache per worker: 330.07 MiB\n",
      "Estimated total cache footprint across workers: 68574.65 MiB\n",
      "Estimated peak dataloader RAM: 77055.65 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_tensor_mb': 33.0,\n",
       " 'inflight_batches': 256,\n",
       " 'BatchTensors_total': 8481.0,\n",
       " 'avg_shard_mb': 14.163182199859687,\n",
       " 'avg_midi_mb': 0.1611671600624968,\n",
       " 'ShardCache_per_worker': 1812.88732158204,\n",
       " 'MidiCache_per_worker': 330.0703438079934,\n",
       " 'Cache_total': 68574.64529248107,\n",
       " 'PeakRAM_total': 77055.64529248107}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params for testing\n",
    "prefetch_factor = 8\n",
    "batch_size = 256\n",
    "num_workers = 32\n",
    "shard_cache_size = 128\n",
    "max_midi_cache_size = 2048\n",
    "temperature = 10/3\n",
    "estimate_peak_loader_ram(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    shard_cache_size=shard_cache_size,\n",
    "    max_midi_cache_size=max_midi_cache_size,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca58a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from data.datasets.loader import build_chunk_dataloader\n",
    "from configs import load_project_config\n",
    "\n",
    "\n",
    "def profile_dataloader(prefetch_factor: int, shard_cache_size: int,max_midi_cache_size: int, num_workers: int, batch_size: int, *,\n",
    "                       max_batches: int = 64, split: str = \"train\", temperature: float = 10/3, log_dir: Path | str = \"dataloader_checkpoints\") -> Path:\n",
    "    \"\"\"Build the chunk dataloader with the provided knobs, print diagnostics, and\n",
    "    dump RAM/time checkpoints to `log_dir`.\n",
    "\n",
    "    Returns the path to the written checkpoint file.\n",
    "    \"\"\"\n",
    "    cfg = load_project_config()\n",
    "    manifest_path = Path(cfg[\"paths\"][\"cache\"][\"chunk_manifest\"])\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f\"Chunk manifest not found at {manifest_path}\")\n",
    "\n",
    "    def available_ram_mib() -> float:\n",
    "        return psutil.virtual_memory().available / 1024**2\n",
    "\n",
    "    loader_kwargs = dict(\n",
    "        manifest_path=manifest_path,\n",
    "        batch_size=batch_size,\n",
    "        feature_type=\"waveform\",\n",
    "        load_tokens=True,\n",
    "        max_examples_per_mix=4,\n",
    "        temperature=temperature,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        compute_log_mel_in_collate=False,\n",
    "        collate_device=\"cpu\",\n",
    "        seed=0,\n",
    "        shard_cache_size=shard_cache_size,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        max_midi_cache_size=max_midi_cache_size,\n",
    "        split=split,\n",
    "    )\n",
    "\n",
    "    t_build = time.perf_counter()\n",
    "    dataloader = build_chunk_dataloader(**loader_kwargs)\n",
    "    print(f\"Built dataloader with {loader_kwargs} in {time.perf_counter() - t_build:.2f}s\")\n",
    "\n",
    "    base_ram = available_ram_mib()\n",
    "    batch_times: list[float] = []\n",
    "    batch_ram: list[float] = []\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        load_time = time.perf_counter() - t0\n",
    "        delta_ram = base_ram - available_ram_mib()\n",
    "        batch_times.append(load_time)\n",
    "        batch_ram.append(delta_ram)\n",
    "\n",
    "        print(f\"[Batch {idx}] load={load_time:.2f}s, tokens={batch['tokens'].shape}, \"\n",
    "              f\"waveform={batch['waveform'].shape}, ΔRAM={delta_ram:.2f} MiB\")\n",
    "        if idx + 1 >= max_batches:\n",
    "            break\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    out_dir = Path(log_dir)\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    out_path = out_dir / f\"dataloader_checkpoints_bs{batch_size}_nw{num_workers}_scs{shard_cache_size}_pr{prefetch_factor}.txt\"\n",
    "    with out_path.open(\"w\") as fp:\n",
    "        fp.write(f\"Loader kwargs: {loader_kwargs}\\n\")\n",
    "        for i, (ram, t) in enumerate(zip(batch_ram, batch_times)):\n",
    "            fp.write(f\"Batch {i}: Available RAM: {ram:.2f} MiB, Load time: {t:.2f} seconds\\n\")\n",
    "        fp.write(f\"Peak memory usage during batch loading: {max(batch_ram, default=0):.2f} MiB\\n\")\n",
    "        fp.write(f\"Min batch load time: {min(batch_times, default=0):.2f} seconds\\n\")\n",
    "        fp.write(f\"Max batch load time: {max(batch_times, default=0):.2f} seconds\\n\")\n",
    "        fp.write(f\"Avg batch load time: {(sum(batch_times)/len(batch_times)):.2f} seconds\\n\")\n",
    "    print(f\"Checkpoint saved to {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e7274c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataloader with {'manifest_path': PosixPath('cache/chunk_manifest.parquet'), 'batch_size': 256, 'feature_type': 'waveform', 'load_tokens': True, 'max_examples_per_mix': 4, 'temperature': 3.3333333333333335, 'num_workers': 32, 'pin_memory': True, 'compute_log_mel_in_collate': False, 'collate_device': 'cpu', 'seed': 0, 'shard_cache_size': 128, 'prefetch_factor': 16, 'max_midi_cache_size': 2048, 'split': 'train'} in 12.12s\n",
      "[Batch 0] load=45.71s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47719.47 MiB\n",
      "[Batch 1] load=2.31s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47898.15 MiB\n",
      "[Batch 2] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47898.08 MiB\n",
      "[Batch 3] load=0.41s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47838.77 MiB\n",
      "[Batch 4] load=0.52s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47809.16 MiB\n",
      "[Batch 5] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47806.70 MiB\n",
      "[Batch 6] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47804.05 MiB\n",
      "[Batch 7] load=0.19s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47937.23 MiB\n",
      "[Batch 8] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47937.23 MiB\n",
      "[Batch 9] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47937.23 MiB\n",
      "[Batch 10] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47937.72 MiB\n",
      "[Batch 11] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47937.72 MiB\n",
      "[Batch 12] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47938.46 MiB\n",
      "[Batch 13] load=0.08s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47918.14 MiB\n",
      "[Batch 14] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47918.14 MiB\n",
      "[Batch 15] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47918.14 MiB\n",
      "[Batch 16] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47918.14 MiB\n",
      "[Batch 17] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47918.14 MiB\n",
      "[Batch 18] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47919.12 MiB\n",
      "[Batch 19] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47919.12 MiB\n",
      "[Batch 20] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47919.12 MiB\n",
      "[Batch 21] load=0.21s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47973.24 MiB\n",
      "[Batch 22] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47972.75 MiB\n",
      "[Batch 23] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47971.75 MiB\n",
      "[Batch 24] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47970.27 MiB\n",
      "[Batch 25] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47968.86 MiB\n",
      "[Batch 26] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47969.35 MiB\n",
      "[Batch 27] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47969.35 MiB\n",
      "[Batch 28] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47969.35 MiB\n",
      "[Batch 29] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47968.86 MiB\n",
      "[Batch 30] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47969.60 MiB\n",
      "[Batch 31] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=47969.11 MiB\n",
      "[Batch 32] load=40.57s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48892.14 MiB\n",
      "[Batch 33] load=5.08s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49116.41 MiB\n",
      "[Batch 34] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49115.42 MiB\n",
      "[Batch 35] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49117.91 MiB\n",
      "[Batch 36] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49117.91 MiB\n",
      "[Batch 37] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49117.91 MiB\n",
      "[Batch 38] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49117.42 MiB\n",
      "[Batch 39] load=1.75s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48960.46 MiB\n",
      "[Batch 40] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48960.17 MiB\n",
      "[Batch 41] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.68 MiB\n",
      "[Batch 42] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.68 MiB\n",
      "[Batch 43] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.68 MiB\n",
      "[Batch 44] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.68 MiB\n",
      "[Batch 45] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.68 MiB\n",
      "[Batch 46] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.43 MiB\n",
      "[Batch 47] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.18 MiB\n",
      "[Batch 48] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 49] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 50] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 51] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 52] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 53] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 54] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48959.61 MiB\n",
      "[Batch 55] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48958.63 MiB\n",
      "[Batch 56] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48955.43 MiB\n",
      "[Batch 57] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.98 MiB\n",
      "[Batch 58] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 59] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 60] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 61] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 62] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 63] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=48954.73 MiB\n",
      "[Batch 64] load=39.46s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=49658.90 MiB\n",
      "[Batch 65] load=7.72s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50054.07 MiB\n",
      "[Batch 66] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50054.07 MiB\n",
      "[Batch 67] load=5.15s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50231.99 MiB\n",
      "[Batch 68] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50231.99 MiB\n",
      "[Batch 69] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50229.99 MiB\n",
      "[Batch 70] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50228.02 MiB\n",
      "[Batch 71] load=2.08s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.31 MiB\n",
      "[Batch 72] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.31 MiB\n",
      "[Batch 73] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.80 MiB\n",
      "[Batch 74] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.80 MiB\n",
      "[Batch 75] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.80 MiB\n",
      "[Batch 76] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.80 MiB\n",
      "[Batch 77] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50195.29 MiB\n",
      "[Batch 78] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50195.79 MiB\n",
      "[Batch 79] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50196.03 MiB\n",
      "[Batch 80] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50195.78 MiB\n",
      "[Batch 81] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50195.78 MiB\n",
      "[Batch 82] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50196.02 MiB\n",
      "[Batch 83] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50194.29 MiB\n",
      "[Batch 84] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50192.82 MiB\n",
      "[Batch 85] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50200.38 MiB\n",
      "[Batch 86] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50200.98 MiB\n",
      "[Batch 87] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50200.00 MiB\n",
      "[Batch 88] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50200.00 MiB\n",
      "[Batch 89] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.88 MiB\n",
      "[Batch 90] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.88 MiB\n",
      "[Batch 91] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.58 MiB\n",
      "[Batch 92] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.58 MiB\n",
      "[Batch 93] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.52 MiB\n",
      "[Batch 94] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.52 MiB\n",
      "[Batch 95] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50199.52 MiB\n",
      "[Batch 96] load=28.47s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50991.65 MiB\n",
      "[Batch 97] load=10.37s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51073.71 MiB\n",
      "[Batch 98] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51073.96 MiB\n",
      "[Batch 99] load=4.87s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50930.62 MiB\n",
      "[Batch 100] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50928.62 MiB\n",
      "[Batch 101] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50926.90 MiB\n",
      "[Batch 102] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=50925.90 MiB\n",
      "[Batch 103] load=2.24s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51026.68 MiB\n",
      "[Batch 104] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51027.17 MiB\n",
      "[Batch 105] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51027.17 MiB\n",
      "[Batch 106] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51027.17 MiB\n",
      "[Batch 107] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51027.17 MiB\n",
      "[Batch 108] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51027.11 MiB\n",
      "[Batch 109] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51025.64 MiB\n",
      "[Batch 110] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51019.73 MiB\n",
      "[Batch 111] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.91 MiB\n",
      "[Batch 112] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.43 MiB\n",
      "[Batch 113] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.86 MiB\n",
      "[Batch 114] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51013.11 MiB\n",
      "[Batch 115] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51013.11 MiB\n",
      "[Batch 116] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51013.11 MiB\n",
      "[Batch 117] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51013.11 MiB\n",
      "[Batch 118] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51013.11 MiB\n",
      "[Batch 119] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 120] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 121] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 122] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 123] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 124] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51012.62 MiB\n",
      "[Batch 125] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51011.14 MiB\n",
      "[Batch 126] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51010.15 MiB\n",
      "[Batch 127] load=0.00s, tokens=torch.Size([256, 1024]), waveform=torch.Size([256, 32768]), ΔRAM=51010.15 MiB\n",
      "Checkpoint saved to dataloader_checkpoints/dataloader_checkpoints_bs256_nw32_scs128_pr16.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataloader_checkpoints/dataloader_checkpoints_bs256_nw32_scs128_pr16.txt')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params for testing\n",
    "prefetch_factor = 16\n",
    "batch_size = 256\n",
    "num_workers = 32\n",
    "shard_cache_size = 128\n",
    "max_midi_cache_size = 2048\n",
    "temperature = 10/3\n",
    "max_batches = 128\n",
    "profile_dataloader(\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    shard_cache_size=shard_cache_size,\n",
    "    max_midi_cache_size=max_midi_cache_size,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature,\n",
    "    max_batches=max_batches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c62e7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from configs import load_project_config\n",
    "from data.datasets.loader import build_chunk_dataloader\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "\n",
    "def get_available_memory_mib():\n",
    "    \"\"\"Helper function to return available memory in MiB.\"\"\"\n",
    "    return psutil.virtual_memory().available / (1024 ** 2)\n",
    "\n",
    "print(\"--- Script Start ---\")\n",
    "\n",
    "\n",
    "cfg = load_project_config()\n",
    "manifest_path = Path(cfg[\"paths\"][\"cache\"][\"chunk_manifest\"])\n",
    "if not manifest_path.exists():\n",
    "    raise FileNotFoundError(f\"Chunk manifest not found at {manifest_path}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "loader_kwargs = dict(\n",
    "    manifest_path=manifest_path,\n",
    "    batch_size=16,\n",
    "    feature_type=\"waveform\",\n",
    "    load_tokens=True,\n",
    "    max_examples_per_mix=4,\n",
    "    temperature=0.3,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    compute_log_mel_in_collate=False,\n",
    "    collate_device=\"cpu\",\n",
    "    seed=0,\n",
    "    shard_cache_size=32,\n",
    "    prefetch_factor=4,\n",
    "    max_midi_cache_size=2048,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "dataloader = build_chunk_dataloader(**loader_kwargs) \n",
    "print(f\"Building dataloader in {time.time() - start_time:.2f} seconds...\")\n",
    "# 1. Capture the initial available RAM\n",
    "initial_available_ram = get_available_memory_mib()\n",
    "checkpoint_1 = initial_available_ram - get_available_memory_mib() \n",
    "\n",
    "print(\"\\n--- DataLoader Checkpoint ---\")\n",
    "print(f\"used RAM after building dataloader: {checkpoint_1:.2f} MiB\")\n",
    "\n",
    "batch_checkpoints_ram = []\n",
    "batch_checkpoints_time = []\n",
    "\n",
    "t0 = time.time()\n",
    "batch_number = 64\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(f\"Batch {idx+1}\")\n",
    "    print(\"batch keys:\", batch.keys())\n",
    "    print(\"  waveform:\", batch[\"waveform\"].shape)\n",
    "    print(\"  tokens:\", batch[\"tokens\"].shape)\n",
    "    # token list sample examples (first 10 tokens of the first sample in the batch)\n",
    "    print(\"  token sample (first 10 tokens of first sample):\", batch[\"tokens\"][0][:10])\n",
    "    # token list sample examples (last 10 tokens of the first sample in the batch)\n",
    "    print(\"  token sample (last 10 tokens of first sample):\", batch[\"tokens\"][0][-10:])\n",
    "    print(\"token mask:\", batch[\"token_mask\"].shape)\n",
    "    print(\"  metadata :\", batch[\"metadata\"])\n",
    "    t1 = time.time() - t0\n",
    "    print(f\"Batch {idx+1} loaded in {t1:.2f} seconds\")\n",
    "    batch_checkpoints_time.append(t1)\n",
    "    batch_checkpoints_ram.append(initial_available_ram - get_available_memory_mib() )\n",
    "    t0 = time.time()\n",
    "    if idx >= batch_number - 1:\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"total time: {total_time:.2f} seconds\")\n",
    "# Save time and ram checkpoints to a file with the loader_kwargs info and the file name should include the batch size and num_workers and shard_cache_size, all the files should be inside a folder called dataloader_checkpoints\n",
    "output_dir = Path(\"dataloader_checkpoints\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_path = output_dir / f\"dataloader_checkpoints_bs{loader_kwargs['batch_size']}_nw{loader_kwargs['num_workers']}_scs{loader_kwargs['shard_cache_size']}_pr{loader_kwargs['prefetch_factor']}.txt\"\n",
    "with output_path.open(\"w\") as f:\n",
    "    f.write(f\"Loader kwargs: {loader_kwargs}\\n\")\n",
    "    for i, (ram, t) in enumerate(zip(batch_checkpoints_ram, batch_checkpoints_time)):\n",
    "        f.write(f\"Batch {i}: Available RAM: {ram:.2f} MiB, Load time: {t:.2f} seconds\\n\")\n",
    "    # peak memory usage during the batch loading\n",
    "    peak_memory = max(batch_checkpoints_ram)\n",
    "    # min, max, avg loading time\n",
    "    min_time = min(batch_checkpoints_time)\n",
    "    max_time = max(batch_checkpoints_time)\n",
    "    avg_time = sum(batch_checkpoints_time) / len(batch_checkpoints_time)\n",
    "    f.write(f\"Peak memory usage during batch loading: {peak_memory:.2f} MiB\\n\")\n",
    "    f.write(f\"Min batch load time: {min_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Max batch load time: {max_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Avg batch load time: {avg_time:.2f} seconds\\n\")       \n",
    "print(f\"Checkpoints saved to {output_path}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4b11a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt3-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
